{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+EuVzEKKUxLwfr78oGajk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asifsiraz/ML-projects/blob/main/Sparse_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, datasets\n",
        "from tensorflow.keras.regularizers import l1  # Import the L1 regularization\n",
        "\n",
        "# Load MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Define the model with batch normalization and ReLU activation, and L1 regularization\n",
        "def create_model_with_l1():\n",
        "    model = models.Sequential([\n",
        "        layers.Flatten(input_shape=(28, 28)),\n",
        "        layers.Dense(256, activation='relu', kernel_initializer='he_normal', activity_regularizer =l1(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(128, activation='relu', kernel_initializer='he_normal', activity_regularizer=l1(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(64, activation='relu', kernel_initializer='he_normal', activity_regularizer=l1(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(10, activation='softmax',activity_regularizer=l1(0.001))\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Train the model with L1 regularization\n",
        "model_with_l1 = create_model_with_l1()\n",
        "model_with_l1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_with_l1.fit(train_images, train_labels, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Function to calculate the percentage of active nodes in each layer using forward pass\n",
        "def calculate_active_nodes(model, x):\n",
        "    active_nodes_counts = []\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, layers.Dense):\n",
        "            # Perform forward pass\n",
        "            x = layer(x)\n",
        "            # Identify active nodes (non-zero activations) for the current layer\n",
        "            active_nodes = np.sum(np.any(x.numpy() != 0,axis=0))\n",
        "            active_nodes_counts.append(active_nodes)\n",
        "    return active_nodes_counts\n",
        "\n",
        "# Create a sample batch for the forward pass\n",
        "sample_batch_flattened = train_images[:64].reshape(64, -1)\n",
        "\n",
        "# Calculate the number of active nodes for each layer using forward pass\n",
        "active_nodes_counts = calculate_active_nodes(model_with_l1, sample_batch_flattened)\n",
        "for i, count in enumerate(active_nodes_counts, start=1):\n",
        "    print(f\"Layer {i}: {count} active nodes\")\n",
        "\n",
        "def calculate_active_nodes_percentages(model, x):\n",
        "    active_nodes_Counts=[]\n",
        "    active_nodes_percentages = []\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, layers.Dense):\n",
        "            # Perform forward pass\n",
        "            x = layer(x)\n",
        "            # Identify active nodes (non-zero activations) for the current layer\n",
        "            active_nodes = np.sum(np.any(x.numpy() != 0, axis=0))\n",
        "            total_nodes = x.shape[-1]\n",
        "            active_nodes_percentage = (active_nodes / total_nodes) * 100\n",
        "            active_nodes_percentages.append(active_nodes_percentage)\n",
        "\n",
        "    return active_nodes_percentages\n",
        "\n",
        "\n",
        "active_nodes_percentages = calculate_active_nodes_percentages(model_with_l1, sample_batch_flattened)\n",
        "for i, percentage in enumerate(active_nodes_percentages, start=1):\n",
        "    print(f\"Layer {i}: {percentage:.2f}% active nodes\")\n",
        "\n",
        "\n",
        "def calculate_sparsity(model):\n",
        "    total_params = 0\n",
        "    zero_params = 0\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, layers.Dense):\n",
        "            # Get the layer's weights\n",
        "            weights = layer.get_weights()[0]\n",
        "            total_params += np.prod(weights.shape)\n",
        "            zero_params += np.sum(np.abs(weights) < 1e-5)  # Count the number of near-zero weights\n",
        "    sparsity = zero_params / total_params * 100\n",
        "    return sparsity\n",
        "\n",
        "# Calculate the sparsity of the model\n",
        "sparsity = calculate_sparsity(model_with_l1)\n",
        "print(f\"Model Sparsity: {sparsity:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjToCuXf0wfr",
        "outputId": "ecffe630-ff59-426c-8ad9-a348f6ab3992"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 7s 7ms/step - loss: 0.3968 - accuracy: 0.9242 - val_loss: 0.2595 - val_accuracy: 0.9554\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 5s 7ms/step - loss: 0.2277 - accuracy: 0.9609 - val_loss: 0.2311 - val_accuracy: 0.9637\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 5s 7ms/step - loss: 0.1885 - accuracy: 0.9671 - val_loss: 0.2020 - val_accuracy: 0.9623\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 5s 6ms/step - loss: 0.1637 - accuracy: 0.9706 - val_loss: 0.1904 - val_accuracy: 0.9675\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 6s 8ms/step - loss: 0.1474 - accuracy: 0.9746 - val_loss: 0.1861 - val_accuracy: 0.9657\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 5s 6ms/step - loss: 0.1374 - accuracy: 0.9759 - val_loss: 0.1734 - val_accuracy: 0.9677\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 6s 8ms/step - loss: 0.1276 - accuracy: 0.9779 - val_loss: 0.1798 - val_accuracy: 0.9630\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 5s 6ms/step - loss: 0.1220 - accuracy: 0.9794 - val_loss: 0.1718 - val_accuracy: 0.9665\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 5s 6ms/step - loss: 0.1119 - accuracy: 0.9813 - val_loss: 0.1738 - val_accuracy: 0.9689\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 5s 7ms/step - loss: 0.1111 - accuracy: 0.9811 - val_loss: 0.1662 - val_accuracy: 0.9686\n",
            "Layer 1: 137 active nodes\n",
            "Layer 2: 128 active nodes\n",
            "Layer 3: 61 active nodes\n",
            "Layer 4: 10 active nodes\n",
            "Layer 1: 53.52% active nodes\n",
            "Layer 2: 100.00% active nodes\n",
            "Layer 3: 95.31% active nodes\n",
            "Layer 4: 100.00% active nodes\n",
            "Model Sparsity: 0.01%\n"
          ]
        }
      ]
    }
  ]
}